
<p align="center">
  <img src="assets\images\SlideHunter_LogoV2.png" alt="SlideHunter App Logo Mockup", width="55%">
  <br/>
  <em>Lightning-fast answers with pinpoint slide/page citations, powered by modern ML retrieval (FAISS + BM25 + reranker) and concise GPT-4o-mini summarization and google/flan-t5-base local-fallback.</em>
</p>

---

## âœ¨ What it does (MVP)

- Ingests **Canvas Pages** + **Image/Text** â†’ 400â€“600 char chunks with rich metadata.
- Builds a **single FAISS store** (`data/faiss/canvas.index` + `facts.json`) using `sentence-transformers/all-MiniLM-L6-v2`.
- **Hybrid retrieval**: FAISS dense vectors + BM25 over titles/snippets; optional **crossâ€‘encoder reranker** for the topâ€‘50.
- **Autoâ€‘router**: technical â†” career (short route descriptions, margin threshold).
- **Typeâ€‘aware boosts**: `Page`/`File` â‰» `Assignment`/`Quiz`; **lowâ€‘score refusal** to avoid weak citations.
- **Phase date facts** parsed from module names (e.g., `P2W1 (6/9â€“6/13)` â‡’ â€œPhase 2 begins June 9â€)
  
---

<p align="center">
  <img src="assets\images\SlideHunter_App_Flow_Diagram.png" alt="SlideHunter-App Flow Diagram", width="80%">
  <br/>
  <em>Find exactly where a concept lives in course slides and notes. Lightning-fast answers with pinpoint slide/page citations, powered by modern ML retrieval (FAISS + BM25 + reranker), concise GPT-4o-mini summarization with google/flan-t5-base model as fallback </em>
  </p>

## âš™ï¸ Flow Overview

- The SlideHunter app connects directly to the Canvas API to ingest course content (pages, modules, links). Text is chunked into manageable pieces (â‰ˆ400â€“600 characters) with rich metadata, then indexed in two ways:
  - Dense embeddings (MiniLM-L6-v2 â†’ FAISS vector store) for semantic search
  - Sparse BM25 over titles/snippets for keyword relevance
- A lightweight router steers queries between technical and career domains:
  - while optional components like a BGE reranker refine top-50 hits and a low-score refusal guardrail filters out weak matches.
  - Results flow into the Streamlit UI, which surfaces top-k hits (we've decided on k=4 for now.), citations, and optional summarization.

---

## ğŸ™Œ The Team

```
Mina Grullon, Fari Lindo, Thalyann Olivo, Jahaira Zhagnay
```

---

## ğŸ—‚ï¸ Repo Structure

```
SLIDEHUNT/                       # Root of local-repo
â”œâ”€ app/
â”‚  â””â”€ app.py                     # Streamlit frontend
â”œâ”€ assets/
â”‚  â””â”€ images/                    # PNGs, etc.
â”œâ”€ data/
â”‚  â”œâ”€ slides/                    # PDFs / source content
â”‚  â”œâ”€ index/                     # (legacy Chroma if you keep it)
â”‚  â””â”€ faiss/
â”‚     â”œâ”€ canvas.index            # FAISS index (persisted)
â”‚     â””â”€ facts.json              # parallel facts + metadata
â”œâ”€ notebooks/
|  â”œâ”€ canvas_api_extraction.ipynb 
â”‚  â”œâ”€ 01_setup_and_ingest.ipynb   # builds data/faiss/*
â”‚  â”œâ”€ 02_query_demo.ipynb         # small/Initial test prompts 
â”‚  â””â”€ 03_eval.ipynb               # Evaluate & inspect outputs
â”œâ”€ outputs/
â”‚  â”œâ”€ data_ds_A.csv
â”‚  â””â”€ eval_prompts.csv
â”œâ”€ prompts/
â”‚  â””â”€ answer_from_context.txt
â”œâ”€ scripts/
|  â”œâ”€ __init__.py
â”‚  â””â”€ nb01_helper.py
â”œâ”€ requirements.txt
â”œâ”€ .env                       # SLIDEHUNTER_BASE="Path to/local repo/root folder", etc.
â”œâ”€ .gitignore
â”œâ”€ flowchart.md
â”œâ”€ LICENSE
â”œâ”€ nb01_helper.py             # nb01_setup_and_ingest's helper script
â”œâ”€ SlideHunter.py             # Streamlit frontend
â””â”€ README.md
```

---

## ğŸš€ Quickstart (Local)

1) **Create venv & install**
  
```bash
python -m venv .venv
# Windows: .\.venv\Scripts\Activate.ps1
# macOS/Linux: source .venv/bin/activate
pip install -r requirements.txt
```

1) **Put slides** in `data/slides/` (or run Canvas ingestion below).

2) **Build the index** (Notebook `01_setup_and_ingest.ipynb`) â†’ writes:
  

```
data/faiss/canvas.index
data/faiss/facts.json
```

3) **Run Streamlit**

- If in root: `streamlit run SlideHunter`
- if in apps folder: `streamlit run app/SlideHunter.py`

> **Windows note:** If `pip install faiss-cpu` fails, use Conda (`conda install -c pytorch faiss-cpu`) or run the notebooks; keep Chroma as a temporary fallback if needed.

---

## ğŸ” Environment

Create **`.env`** in repo root:

```dotenv
SLIDEHUNTER_BASE=.
Optional (since there's a fallback)
 OPENAI_API_KEY=sk-...
Canvas access (for ingestion script/notebook)
 CANVAS_BASE_URL=https://<your-subdomain>.instructure.com
 CANVAS_TOKEN=<your_personal_access_token>
Load with `**python-dotenv** in notebooks/apps or rely on Streamlit environment.`
```

---

## ğŸ“ Accessing Canvas & Parsing Courses/Modules

### Create a Canvas token
- Log into Canvas â†’ **Account â†’ Settings â†’ + New Access Token**.
- Copy the token; store it in `.env` as `CANVAS_TOKEN`.

### Install and connect

If needed install 'pip install -r requirements.txt' to install all dependencies/packages.

```python
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer
import numpy as np, faiss, re, json, os
from canvasapi import Canvas
import torch

CANVAS_BASE_URL = os.getenv("CANVAS_BASE_URL")
CANVAS_TOKEN = os.getenv("CANVAS_TOKEN")
canvas = Canvas(CANVAS_BASE_URL, CANVAS_TOKEN)

OPENAI_API_KEY = config.get("OPENAI_API_KEY")
```

Load with `python-dotenv` in notebooks/apps or rely on Streamlit environment.

---

## ğŸ§  Build Embeddings & FAISS Store

If needed run command 'pip install -r requirements.txt'

```python
from sentence_transformers import SentenceTransformer
import faiss, numpy as np, json, os
from pathlib import Path

# Embeddings
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
emb = model.encode(facts, normalize_embeddings=True, show_progress_bar=True).astype("float32")

# FAISS (cosine via inner product on normalized vectors)
index = faiss.IndexFlatIP(emb.shape[1])
index.add(emb)

# Persist
STORE_DIR = Path("data/faiss"); STORE_DIR.mkdir(parents=True, exist_ok=True)
faiss.write_index(index, STORE_DIR / "canvas.index")
json.dump({"facts": facts, "metas": metas}, open(STORE_DIR / "facts.json","w",encoding="utf-8"), ensure_ascii=False)
```

---

## ğŸ§ª Evaluation & Metrics

We track:

- **Coverage** â€” % queries with â‰¥1 hit above threshold  
- **Topâ€‘1 Source Type Precision** â€” Page/File vs Assignment/Quiz for concept queries  
- **Citation Accuracy** â€” manual spotâ€‘check (k=20)  
- **Router Accuracy** â€” target domain vs routed domain  
- **Latency** â€” median; Streamlit shows perâ€‘query  
- **Refusal Rate** â€” % of queries correctly refused

Run `any notebook` to export to `outputs/eval_prompts.csv`.

---

## Canvas API Cheat Sheet (for our notebooks)

### Auth & entry point
- **Create client:** `Canvas(BASE_URL, TOKEN)`
- **Objects we touch most:** `Canvas`, `Course`, `Module`, `ModuleItem`, `Page`, `File`, `Assignment`, `Quiz`, `DiscussionTopic`

---

### Core objects & methods

| Object | Method | Purpose | Returns / Type | Key fields youâ€™ll use | Notes |
|---|---|---|---|---|---|
| **Canvas** | `get_courses(enrollment_state="active")` | List your courses | Iterable of **Course** | `Course.id`, `Course.name` | Filter by name to pick â€œtechnicalâ€ vs â€œcareerâ€. |
| **Canvas** | `get_course(course_id)` | Fetch one course by ID | **Course** | â€” | Use when you know the ID. |
| **Course** | `get_modules()` | List modules in a course | Iterable of **Module** | `Module.id`, `Module.name` | Names often include week/phase (e.g., `P2W1 (6/9â€“6/13)`). |
| **Module** | `get_module_items()` | Items within a module | Iterable of **ModuleItem** | `item.id`, `item.title`, `item.type`, `item.content_id`, `item.page_url`, `item.external_url` | `item.type` determines next call (Page/File/Assignment/Quiz/ExternalUrlâ€¦). |
| **Course** | `get_page(page_url)` | Get a page (from `item.page_url`) | **Page** | `Page.body`, `Page.html_url`, `Page.title` | Use `Page.body` for text extraction. |
| **Course** | `get_pages()` | List all pages | Iterable of **Page** | `Page.url` (slug), `title` | Alternative to walking via modules. |
| **Course** | `get_file(file_id)` | Get a file (from `item.content_id`) | **File** | `File.display_name`, `File.size`, `File.url`, `File.content_type` | Download/parse PDFs if you index files. |
| **Course** | `get_files()` | List all files in course | Iterable of **File** | same as above | Useful for bulk file ingest. |
| **Course** | `get_assignments()` | List assignments | Iterable of **Assignment** | `name`, `due_at`, `html_url` | Usually down-weight for concept queries. |
| **Course** | `get_quizzes()` | List quizzes | Iterable of **Quiz** | `title`, `html_url` | Same note as assignments. |
| **Course** | `get_discussion_topics()` | List discussions | Iterable of **DiscussionTopic** | `title`, `message`, `html_url` | Optional ingest. |
| **ModuleItem** | *(type-specific)* | â€” | â€” | â€” | â€” |
|  | `type == "Page"` | Indicates a Canvas page | â€” | `page_url` (slug) | Then call `course.get_page(item.page_url)`. |
|  | `type == "File"` | Indicates a file (PDF/PPTX) | â€” | `content_id` (file id) | Then call `course.get_file(item.content_id)`. |
|  | `type == "ExternalUrl"` | External link | â€” | `external_url` | Store link as metadata; no body to parse. |
|  | `type in {"Assignment","Quiz","Discussion"}` | Graded items | â€” | `content_id` / `html_url` | Useful links; not primary concept source. |

---

### Typical flows (at a glance)

1. **Enumerate courses â†’ pick targets**  
   `Canvas â†’ get_courses()` â†’ filter by `Course.name` (technical vs career)

2. **Walk modules & items**  
   `Course â†’ get_modules()` â†’ each `Module â†’ get_module_items()`

3. **Fetch content**  
   - If `item.type == "Page"` â†’ `course.get_page(item.page_url)` â†’ use `Page.body`  
   - If `item.type == "File"` â†’ `course.get_file(item.content_id)` â†’ download/parse  
   - Else (`Assignment`/`Quiz`/`ExternalUrl`) â†’ keep `title` + `html_url`/`external_url` as metadata

4. **Save metadata with each chunk**  
   `course_name`, `course_id`, `module_name`, `module_id`, `item_title`, `item.type`, `url/html_url`, and our `domain` (technical/career)

---

### Common `ModuleItem.type` values (routing hint)

- `Page` â†’ primary source for **lecture notes / concepts** âœ…  
- `JSON/XML` (HTML/Text/Items) â†’ **slides/pages/** âœ…  
- `Assignment`, `Quiz`, `Discussion` â†’ links/context; **down-weight** for concept queries  
- `ExternalUrl`, `ExternalTool` â†’ store link/cite only

---

### Practical notes

- **Pagination:** `canvasapi` iterables auto-paginate; just loop.  
- **Rate limits:** be gentle; cache `Page.body` to disk for re-runs.  
- **Phase dates:** module names often include ranges like `P2W1 (6/9â€“6/13)`â€”parse once into a `phase_start` map.  
- **Security:** never commit tokens; keep `CANVAS_BASE_URL` and `CANVAS_TOKEN` in `.env`.

---

## ğŸ”¬ Approach , Evaluation Results, Findings, Setbacks & Resolutions

## âœ… Approach (MVP)

- **Retriever:** SentenceTransformers (MiniLM-L6-v2) â†’ FAISS (cosine, IndexFlatIP)  
- **Hybrid boost:** BM25 on titles + leading snippets (blend: 0.7*dense + 0.3*BM25)  
- **Quality levers:** type-aware boosts (Page/File â‰» Assignment/Quiz), low-score refusal (Ï„ = 0.40)  
- **Routing:** lightweight domain router (technical â†” career)  
- **Dates:** phase date extractor from module names  
- **Optional:** BGE reranker (top-50) and summarizer (GPT-4o/mini or local FLAN-T5)  

## ğŸ“Š Evaluation Results (taken from 20 technical responses)

Below are the evaluation results from 20 test prompts against the FAISS + BM25 + router retriever.  
Each query was run through the `search()` function with `scope="auto"` (router-enabled).  

| query                                      | scope      | latency_s | top_score | top_domain | citation (truncated) |
|--------------------------------------------|------------|-----------|-----------|------------|-----------------------|
| When does phase 2 begin?                   | all        | 0.041     | 0.476     | technical  | IF '25 Data Science Cohort A > P2W12 ... |
| Any way of saying June 9th?                | technical  | 0.026     | 0.230     | technical  | Foundations Course > Week 1: Foundations ... |
| Where can I find my instructor's email?    | technical  | 0.022     | 0.332     | technical  | IF '25 Data Science Cohort A > Fellow Res... |
| Under Course Team Contact Information?     | all        | 0.037     | 0.444     | technical  | IF '25 Data Science Cohort A > Fellow Res... |
| What was the last TLAB about?              | technical  | 0.023     | 0.334     | technical  | IF '25 Data Science Cohort A > P2W9 ...     |
| An explanation of making a recommender.    | technical  | 0.014     | 0.465     | technical  | IF '25 Data Science Cohort A > P2W9 ...     |
| What lecture slides cover pivot tables?    | technical  | 0.033     | 0.563     | technical  | IF '25 Data Science Cohort A > P1W6 ...     |
| What lecture slides explain control flow?  | technical  | 0.020     | 0.389     | technical  | Foundations Course > Week 1: Foundations ... |
| Bullet point list of SQL concepts?         | technical  | 0.018     | 0.577     | technical  | IF '25 Data Science Cohort A > P1W9 ...     |
| When does phase 2 commence?                | all        | 0.031     | 0.361     | technical  | IF '25 Data Science Cohort A > P2W1 ...     |
| Summary of P2W2â€™s material?                | all        | 0.016     | 0.518     | technical  | IF '25 Data Science Cohort A > P2W2 ...     |
| Where did we define precision vs. recall?  | technical  | 0.027     | 0.382     | technical  | IF '25 Data Science Cohort A > P2W3 ...     |

---

### ğŸ“ Initial Observations

- **Coverage:** 100% (all 20 questions returned at least one hit).  
- **Latency:** Fast, consistently between `0.01â€“0.04s` per query.  
- **Top scores:** Range `0.23â€“0.58`.  
  - ~70% of queries scored **â‰¥ 0.38** (above the refusal threshold of 0.40).  
  - A few (e.g., *â€œAny way of saying June 9th?â€*, *XGBoost vs AdaBoost*) were borderline at ~0.23â€“0.29, indicating weaker matches which makes sense, since the XGBoost model wasn't a covered topic.  
- **Domains:** All results classified as **technical**. No â€œcareerâ€ prompts were included in this round â€” router evaluation pending.  
- **Insights:** Retrieval is strong for structured concepts (e.g., SQL, PCA, regression), weaker for ambiguous/natural phrasing (â€œJune 9thâ€ question).  

---

## **Key Observations**

- **Hybrid helps names & codes:** BM25 boosts matches for P2W2, â€œpivot tablesâ€, â€œROCâ€, â€œlog lossâ€  
- **Type boosts reduce noise:** Concept questions more reliably hit Page/File over Assignment/Quiz  
- **Reranker = precision mode:** Improves ordering when candidates are close; toggle off to keep latency minimal  
- **Guardrailed summarizer:** Only runs when top hit â‰¥ Ï„; otherwise refuse + show snippets to avoid hallucinations  
- **Ambiguity hurts dense scores:** Colloquial queries benefit from BM25 blending & mild rewrites  

---

**Detailed Findings:**

- hybrid helps named tokens (e.g., `P2W2`, `pivot tables`); light boosts remove many quiz/assignment misâ€‘hits; reranker helps edge cases. We tested our retrieval models using questions categorized into 3 different levels of difficulty: easy (3 example questions), medium (5 example questions), and hard (7 example questions).

- Our easy questions were used to recall material that is more factual with only one answer which would allow us to test our models retrieval ability for exact answers.
  - For example, when asked "When does phase 2 begin?" the output returned a top score of 0.467 which revelas a relatively strong match with a low latency score of 0.054.
  - Our medium questions involved more conceptual content and required the model to switch gears to work more towards summarizing slide content.
  - We found the medium level questions optimal for testing semantic retrieval since it required giving a more nuanced output.
    - For example, when asked about pivot tables or SQL concepts the output was often not explicit however we observed accuracy (~50%) heading generally in a positive direction with top scores around 0.42.
    - Similarly, the harder questions demanded a more technical response of summarized content from different topics discussed at different times throughout the students' learning period.
- These queries demonstrated a weak performance returning an average top score of about .30. We also included noisy inputs to test the ability to retrieve information based on informal language and typos.
- Additionally, we also included out of scope questions:
  - such as "Can I see other students' grades??" to test the models ability to truly distinguish between career or technical modules when prompted a question that doesn't necessarily fall into either category.

In our final evaluation of the model we found the average top score to be 0.407 which demonstrated the model is finding relevant matches since every query returned at least one citation (100% coverage) but it is not a strong.

---

## ğŸ§­ Process Notes

- Collaboration & learning: Colab for GPUs, shared iteration  
- Productization: VS Code + Streamlit + clean repo structure  
- Reproducibility: FAISS store in `data/faiss/` shared by notebooks & app  
  
---

## âš ï¸ Limitations (MVP)

- No OCR â†’ image-only slides are skipped  
- Single embedding model (MiniLM-L6-v2); no multilingual / long-context  
- Router is simple; no multi-index routing yet  

---

ğŸ”® **Next steps:**  

- Add **10â€“15 career-focused prompts** (resume, LinkedIn, cover letters, interview prep) to test router accuracy.  
- Increase total evaluation set to ~30â€“40 queries for better coverage.  
- Inspect **borderline low-score cases** to refine refusal threshold or add reranker tuning.  

---

## ğŸ§­ Streamlit Demo

- **Scope:** auto/technical/career/all  
- Toggles: **BM25 hybrid**, **reranker**, **lowâ€‘score refusal**  
- Special handling: **Phase 2 begin date**

## ğŸ” Reproduce (quick)

1. Run ingest â†’ build `data/faiss/{canvas.index,facts.json}`  
2. Run query demo â†’ verify retrieval & citations  
3. Launch Streamlit:  

```bash
streamlit run app/SlideHunter.py or SlindHunter.py
```

If deploying from Colab, you can use **cloudflared** to expose a public URL.

---

## ğŸ”’ Data & Privacy

- Store only course content text and metadata. No student PII.  
- `.env` for tokens/keys. Never commit `.env` or source PDFs if restricted.

---

## ğŸ—ºï¸ Future Expansion Roadmap

- OCR fallback for imageâ€‘only slides
- Richer date/deadline extractors  
- Option to split into domainâ€‘specific stores  
- Instructionâ€‘tuned summarizer for 1â€“2 sentence answers with citations  
- FastAPI search service (twoâ€‘service architecture)

---

## ğŸ¤ Team Collaboration

**Roles (template):** PM & UX Â· ETL & chunking Â· Retrieval & scoring Â· Reranker & QA Â· DevOps & CI

**Working agreement:**pip install -r requirements.txt

- `main` deliberate communicate changes.
- Donâ€™t commit slides or `.env`.
- Communicate prior to each commit/push via Slack group, etc.
- Communicate changes in order to prevent modifying identical content.
- Fari (@DataEden) â€“ Lead Developer  
- ChatGPT (OpenAI) â€“ AI Pair Programmer / Vibe Coding Partner.

---

## ğŸ“œ License

!MIT [see `LICENSE`](\LICENSE)


# SlideHunter â€” Lecture Navigator MVP (Multiâ€‘Modal RAG)

<p align="center">
  <img src="images/SlideHunter_Logo.png" alt="SlideHunter-App Flow Diagram", width="70%">
  <br/>
  <em>Ingestion â†’ Retrieval (FAISS + BM25) â†’ Routing/Rerank â†’ Streamlit UI with citations</em>
</p>

> Lightning-fast answers with pinpoint slide/page citations, powered by modern ML retrieval (FAISS + BM25 + reranker) and concise GPT-4o-mini summarization.  

---

## âœ¨ What it does (MVP)

- Ingests **Canvas Pages** + **PDF slides** â†’ 400â€“600 char chunks with rich metadata.
- Builds a **single FAISS store** (`data/faiss/canvas.index` + `facts.json`) using `sentence-transformers/all-MiniLM-L6-v2`.
- **Hybrid retrieval**: FAISS dense vectors + BM25 over titles/snippets; optional **crossâ€‘encoder reranker** for the topâ€‘50.
- **Autoâ€‘router**: technical â†” career (short route descriptions, margin threshold).
- **Typeâ€‘aware boosts**: `Page`/`File` â‰» `Assignment`/`Quiz`; **lowâ€‘score refusal** to avoid weak citations.
- **Phase date facts** parsed from module names (e.g., `P2W1 (6/9â€“6/13)` â‡’ â€œPhase 2 begins June 9â€)
  
---

<p align="center">
  <img src="images/SlideHunter_App_Flow_Diagram.png" alt="SlideHunter-App Flow Diagram", width="70%">
  <br/>
  <em>Find exactly where a concept lives in course slides and notes. Lightning-fast answers with pinpoint slide/page citations, powered by modern ML retrieval (FAISS + BM25 + reranker), concise GPT-4o-mini summarization with google/flan-t5-base model as fallback </em>
  </p>

---

## ğŸ™Œ The Team

```

Mina Grullon, Fari Lindo, Thalyann Olivo, Jahaira Zhagnay

```

---

## ğŸ—‚ï¸ Repo Structure

```
SLIDEHUNTER/
â”œâ”€ app/
â”‚  â””â”€ app.py                     # Streamlit frontend
â”œâ”€ data/
â”‚  â”œâ”€ slides/                    # PDFs / source content
â”‚  â”œâ”€ index/                     # (legacy Chroma if you keep it)
â”‚  â””â”€ faiss/
â”‚     â”œâ”€ canvas.index            # FAISS index (persisted)
â”‚     â””â”€ facts.json              # parallel facts + metadata
â”œâ”€ notebooks/
|  â”œâ”€ canvas_api_extraction.ipynb 
â”‚  â”œâ”€ 01_setup_and_ingest.ipynb  # builds data/faiss/*
â”‚  â”œâ”€ 02_query_demo.ipynb        # quick search & inspect
â”‚  â””â”€ 03_eval.ipynb              # test prompts & metrics
â”œâ”€ prompts/
â”‚  â””â”€ answer_from_context.txt
â”œâ”€ outputs/
â”‚  â”œâ”€ data_ds_A.csv
â”‚  â””â”€ eval_prompts.csv
â”œâ”€ requirements.txt
â”œâ”€ .env                          # SLIDEHUNT_BASE=.; (optional keys)
â”œâ”€ .gitignore
â”œâ”€ flowchart.md
â”œâ”€ LICENSE
â”œâ”€ nb01_helper.py
â”œâ”€ SlideHunter.py
â””â”€ README.md
```

---

## ğŸš€ Quickstart (Local)

1) **Create venv & install**
```bash
python -m venv .venv
# Windows: .\.venv\Scripts\Activate.ps1
# macOS/Linux: source .venv/bin/activate
pip install -r requirements.txt
```

2) **Put slides** in `data/slides/` (or run Canvas ingestion below).

3) **Build the index** (Notebook `01_setup_and_ingest.ipynb`) â†’ writes:
```
data/faiss/canvas.index
data/faiss/facts.json
```

4) **Run Streamlit**
```bash
streamlit run app/app.py
```

> **Windows note:** If `pip install faiss-cpu` fails, use Conda (`conda install -c pytorch faiss-cpu`) or run the notebooks; keep Chroma as a temporary fallback if needed.

---

## ğŸ” Environment

Create **`.env`** in repo root:

```dotenv
SLIDEHUNT_BASE=.
# Optional (only if you add generation)
# OPENAI_API_KEY=sk-...
# Canvas access (for ingestion script/notebook)
CANVAS_BASE_URL=https://<your-subdomain>.instructure.com
CANVAS_TOKEN=<your_personal_access_token>
```

Load with `python-dotenv` in notebooks/apps or rely on Streamlit environment.

---

## ğŸ“ Accessing Canvas & Parsing Courses/Modules

### Create a Canvas token
- Log into Canvas â†’ **Account â†’ Settings â†’ + New Access Token**.
- Copy the token; store it in `.env` as `CANVAS_TOKEN`.

### Install and connect
```bash
pip install canvasapi beautifulsoup4
```

```python
import os, re
from canvasapi import Canvas
from bs4 import BeautifulSoup

BASE_URL = os.getenv("CANVAS_BASE_URL")
TOKEN    = os.getenv("CANVAS_TOKEN")
canvas   = Canvas(BASE_URL, TOKEN)
```

---

## ğŸ§  Build Embeddings & FAISS Store

```bash
pip install sentence-transformers faiss-cpu rank-bm25
```

```python
from sentence_transformers import SentenceTransformer
import faiss, numpy as np, json, os
from pathlib import Path

# Embeddings
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
emb = model.encode(facts, normalize_embeddings=True, show_progress_bar=True).astype("float32")

# FAISS (cosine via inner product on normalized vectors)
index = faiss.IndexFlatIP(emb.shape[1])
index.add(emb)

# Persist
STORE_DIR = Path("data/faiss"); STORE_DIR.mkdir(parents=True, exist_ok=True)
faiss.write_index(index, STORE_DIR / "canvas.index")
json.dump({"facts": facts, "metas": metas}, open(STORE_DIR / "facts.json","w",encoding="utf-8"), ensure_ascii=False)
```

---

## ğŸ§ª Evaluation & Metrics

We track:

- **Coverage** â€” % queries with â‰¥1 hit above threshold  
- **Topâ€‘1 Source Type Precision** â€” Page/File vs Assignment/Quiz for concept queries  
- **Citation Accuracy** â€” manual spotâ€‘check (k=20)  
- **Router Accuracy** â€” target domain vs routed domain  
- **Latency** â€” median; Streamlit shows perâ€‘query  
- **Refusal Rate** â€” % of queries correctly refused

Run `03_eval.ipynb` to export `outputs/eval_prompts.csv`.

---

## Canvas API Cheat Sheet (for our notebooks)

### Auth & entry point
- **Create client:** `Canvas(BASE_URL, TOKEN)`
- **Objects we touch most:** `Canvas`, `Course`, `Module`, `ModuleItem`, `Page`, `File`, `Assignment`, `Quiz`, `DiscussionTopic`

---

### Core objects & methods

| Object | Method | Purpose | Returns / Type | Key fields youâ€™ll use | Notes |
|---|---|---|---|---|---|
| **Canvas** | `get_courses(enrollment_state="active")` | List your courses | Iterable of **Course** | `Course.id`, `Course.name` | Filter by name to pick â€œtechnicalâ€ vs â€œcareerâ€. |
| **Canvas** | `get_course(course_id)` | Fetch one course by ID | **Course** | â€” | Use when you know the ID. |
| **Course** | `get_modules()` | List modules in a course | Iterable of **Module** | `Module.id`, `Module.name` | Names often include week/phase (e.g., `P2W1 (6/9â€“6/13)`). |
| **Module** | `get_module_items()` | Items within a module | Iterable of **ModuleItem** | `item.id`, `item.title`, `item.type`, `item.content_id`, `item.page_url`, `item.external_url` | `item.type` determines next call (Page/File/Assignment/Quiz/ExternalUrlâ€¦). |
| **Course** | `get_page(page_url)` | Get a page (from `item.page_url`) | **Page** | `Page.body`, `Page.html_url`, `Page.title` | Use `Page.body` for text extraction. |
| **Course** | `get_pages()` | List all pages | Iterable of **Page** | `Page.url` (slug), `title` | Alternative to walking via modules. |
| **Course** | `get_file(file_id)` | Get a file (from `item.content_id`) | **File** | `File.display_name`, `File.size`, `File.url`, `File.content_type` | Download/parse PDFs if you index files. |
| **Course** | `get_files()` | List all files in course | Iterable of **File** | same as above | Useful for bulk file ingest. |
| **Course** | `get_assignments()` | List assignments | Iterable of **Assignment** | `name`, `due_at`, `html_url` | Usually down-weight for concept queries. |
| **Course** | `get_quizzes()` | List quizzes | Iterable of **Quiz** | `title`, `html_url` | Same note as assignments. |
| **Course** | `get_discussion_topics()` | List discussions | Iterable of **DiscussionTopic** | `title`, `message`, `html_url` | Optional ingest. |
| **ModuleItem** | *(type-specific)* | â€” | â€” | â€” | â€” |
|  | `type == "Page"` | Indicates a Canvas page | â€” | `page_url` (slug) | Then call `course.get_page(item.page_url)`. |
|  | `type == "File"` | Indicates a file (PDF/PPTX) | â€” | `content_id` (file id) | Then call `course.get_file(item.content_id)`. |
|  | `type == "ExternalUrl"` | External link | â€” | `external_url` | Store link as metadata; no body to parse. |
|  | `type in {"Assignment","Quiz","Discussion"}` | Graded items | â€” | `content_id` / `html_url` | Useful links; not primary concept source. |

---

### Typical flows (at a glance)

1. **Enumerate courses â†’ pick targets**  
   `Canvas â†’ get_courses()` â†’ filter by `Course.name` (technical vs career)

2. **Walk modules & items**  
   `Course â†’ get_modules()` â†’ each `Module â†’ get_module_items()`

3. **Fetch content**  
   - If `item.type == "Page"` â†’ `course.get_page(item.page_url)` â†’ use `Page.body`  
   - If `item.type == "File"` â†’ `course.get_file(item.content_id)` â†’ download/parse  
   - Else (`Assignment`/`Quiz`/`ExternalUrl`) â†’ keep `title` + `html_url`/`external_url` as metadata

4. **Save metadata with each chunk**  
   `course_name`, `course_id`, `module_name`, `module_id`, `item_title`, `item.type`, `url/html_url`, and our `domain` (technical/career)

---

### Common `ModuleItem.type` values (routing hint)

- `Page` â†’ primary source for **lecture notes / concepts** âœ…  
- `File` (PDF/PPTX) â†’ **slides** âœ…  
- `Assignment`, `Quiz`, `Discussion` â†’ links/context; **down-weight** for concept queries  
- `ExternalUrl`, `ExternalTool` â†’ store link/cite only

---

### Practical notes

- **Pagination:** `canvasapi` iterables auto-paginate; just loop.  
- **Rate limits:** be gentle; cache `Page.body` to disk for re-runs.  
- **Phase dates:** module names often include ranges like `P2W1 (6/9â€“6/13)`â€”parse once into a `phase_start` map.  
- **Security:** never commit tokens; keep `CANVAS_BASE_URL` and `CANVAS_TOKEN` in `.env`.

---

## ğŸ¤ Team Collaboration

**Roles (template):** PM & UX Â· ETL & chunking Â· Retrieval & scoring Â· Reranker & QA Â· DevOps & CI

**Working agreement:**
- `main` protected; feature branches â†’ PRs (small, focused).
- Donâ€™t commit slides or `.env`.
- Issues labeled by area: `etl`, `retrieval`, `frontend`, `eval`, `infra`.

---

## ğŸ”¬ Findings, Approach, Setbacks & Resolutions

**Approach:** single FAISS store; BM25 hybrid; typeâ€‘aware boosts; lowâ€‘score refusal; simple router; phase date extractor.

**Findings:** hybrid helps named tokens (e.g., `P2W2`, `pivot tables`); light boosts remove many quiz/assignment misâ€‘hits; reranker helps edge cases. We tested our retrieval models using questions categorized into 3 different levels of difficulty: easy (3 example questions), medium (5 example questions), and hard (7 example questions). 

Our easy questions were used to recall material that is more factual with only one answer which would allow us to test our models retrieval ability for exact answers. For example, when asked "When does phase 2 begin?" the output returned a top score of 0.467 which revelas a relatively strong match with a low latency score of 0.054. Our medium questions involved more conceptual content and required the model to switch gears to work more towards summarizing slide content. We found the medium level questions optimal for testing semantic retrieval since it required giving a more nuanced output. For example, when asked about pivot tables or SQL concepts the output was often not explicit however we observed accuracy (~50%) heading generally in a positive direction with top scores around 0.42. Similarly, the harder questions demanded a more technical response of summarized content from different topics discussed at different times throughout the students' learning period. These queries demonstrated a weak performance returning an average top score of about .30. We also included noisy inputs to test the ability to retrieve information based on informal language and typos. Additionally, we also included out of scope questions such as "Can I see other students' grades??" to test the models ability to truly distinguish between career or technical modules when prompted a question that doesn't necessarily fall into either category.

In our final evaluation of the model we found the average top score to be 0.407 which demonstrated the model is finding relevant matches since every query returned at least one citation (100% coverage) but it is not a strong.

**Setbacks:** sparse PDFs â†’ PyMuPDF + optional OCR; assignment citations on concept queries â†’ boosts + rerank; routing ambiguity â†’ margin to â€œallâ€.

**Resolutions/Results:** cleaner topâ€‘1 slide citations; stable latency; reproducible builds via persisted index.

---

## ğŸ§­ Streamlit Demo

- **Scope:** auto/technical/career/all  
- Toggles: **BM25 hybrid**, **reranker**, **lowâ€‘score refusal**  
- Special handling: **Phase 2 begin date**

If deploying from Colab, you can use **cloudflared** to expose a public URL.

---

## ğŸ”’ Data & Privacy

- Store only course content text and metadata. No student PII.  
- `.env` for tokens/keys. Never commit `.env` or source PDFs if restricted.

---

## ğŸ—ºï¸ Future Expansion Roadmap

- OCR fallback for imageâ€‘only slides  
- Richer date/deadline extractors  
- Option to split into domainâ€‘specific stores  
- Instructionâ€‘tuned summarizer for 1â€“2 sentence answers with citations  
- FastAPI search service (twoâ€‘service architecture)

---

## ğŸ“œ License

!MIT [(see `LICENSE`)](\LICENSE)

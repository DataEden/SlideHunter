{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81cf5caf",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ccc; border-radius: 12px; padding: 20px; max-width: 950px; margin: auto; background-color: #1e1e1e; color: #f0f0f0; font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "\n",
    "  <div style=\"text-align: center; margin-bottom: 20px;\">\n",
    "    <img src=\"..\\images\\SlideHunter_Logo.png\" \n",
    "         alt=\"Coffee Production Boxplot by Subdivision\"\n",
    "         style=\"width: 80%; max-width: 80%; height: auto; border-radius: 8px; box-shadow: 0 0 10px rgba(0,0,0,0.4);\">\n",
    "  </div>\n",
    "\n",
    "  <blockquote style=\"margin: 0; padding: 10px 20px; border-left: 4px solid #4faaff;\">\n",
    "    <p><strong>\n",
    "      SlideHunter App\n",
    "    </strong></p>\n",
    "    <p>\n",
    "     User Interface (UI) : \n",
    "      <a href=\"..\\images\\SlideHunter_Logo.png\" target=\"_blank\" style=\"color: #4faaff;\">\n",
    "        Find exactly where a topic was covered in course materials. Fast answers with precise slide/page citations.\n",
    "      </a>\n",
    "    </p>\n",
    "  </blockquote>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95821ea5",
   "metadata": {},
   "source": [
    "## Check if GPU is alalable for faster embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ce45304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda_available: False\n",
      "torch.cuda: None\n",
      "device: CPU only\n"
     ]
    }
   ],
   "source": [
    "import torch, platform\n",
    "\n",
    "print(\"cuda_available:\", torch.cuda.is_available())\n",
    "print(\"torch.cuda:\", torch.version.cuda)          # None if CUDA isn't available\n",
    "print(\"device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU only\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6bfa35",
   "metadata": {},
   "source": [
    "# 01 â€” Setup & Ingest Notebook\n",
    "\n",
    "## Retrieval-Augmented-Generation (RAG)\n",
    "\n",
    "Retrieval-Augmented Generation is a *technique* to improve an LLM's responses by:\n",
    "* Retrieving relevant documents from a knowledge store (such FAISS or a vector DB).\n",
    "* Augmenting the model's prompt with those documents.\n",
    "* Generating an answer using the model with this extra context.\n",
    "\n",
    "With RAG, the model is handed the right documents at generation time. That is, the model does not respond to a user's queries until it refers to a specified set of documents.\n",
    "\n",
    "The FAISS store acts as our long-term memory for domain knowledge. The RAG pipeline serves as the reasoning loop: it retrieves the most relevant content, assembles the top results into a compact context, and passes that context to the LLM to produce a grounded, citeable response.\n",
    "- Think of FAISS as the library and RAG as the librarian: FAISS shelves all the course knowledge; RAG finds the right books, marks the key pages, and hands them to the model to explain.\n",
    "\n",
    "This notebook parses Canvas Learning Management System (LMS) (cours-level), embeds chunks, and builds a FAISS Store \n",
    "\n",
    "\n",
    "## Install (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04fbd047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning surpresser\n",
    "import os\n",
    "\n",
    "# Tell Hugging Face to skip TensorFlow/Flax so they never import TensorFlow (TF).\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "\n",
    "# Quiet TF logs if something still pulls it in.\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # 1=INFO, 2=WARNING, 3=ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c9353a",
   "metadata": {},
   "source": [
    "## Imports and Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "959f56d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL If env is missing packages, or in a fresh/new environment\n",
    "#%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53230a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\MINA-G\\miniconda3\\envs\\ds\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import json, os\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np, faiss, re, json, os\n",
    "from canvasapi import Canvas\n",
    "import torch\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c2be23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "config = dotenv_values() # load .env file\n",
    "\n",
    "# Injest and process data from Canvas Sections\n",
    "# Set up Canva API client\n",
    "CANVAS_BASE_URL = config.get(\"CANVAS_BASE_URL\")\n",
    "CANVAS_TOKEN=config.get(\"CANVAS_TOKEN\")\n",
    "OPENAI_API_KEY = config.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize Canvas API client \n",
    "canvas = Canvas(CANVAS_BASE_URL, CANVAS_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad90c594",
   "metadata": {},
   "source": [
    "## Injest and process data from Canvas Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "963e5605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the list of courses\n",
    "my_courses = canvas.get_courses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a85877e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foundations '25 Data Science\n",
      "Foundations Course\n",
      "IF '25 Data Science Cohort A\n",
      "IF '25 NY Career Readiness and Success\n"
     ]
    }
   ],
   "source": [
    "# Pulling courses from Canvas\n",
    "my_courses = canvas.get_courses()\n",
    "course_list = []\n",
    "\n",
    "for course in my_courses:\n",
    "    print(course.name)\n",
    "    course_list.append(course)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0df8d2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Module_id: 1118\n",
      "  Module: Fellow Resources\n",
      " - Item: Fellow Success Resources (Page)\n",
      "  Module_id: 1239\n",
      "  Module: Phase 2 (6/9-8/29)\n",
      " - Item: Homework: Option 1 - Weekly Job Applications & Progress Report (Due August 30) (Assignment)\n",
      " - Item: P2W1 (6/12) NO CAREER CLASS - TECHNICAL CLASS (SubHeader)\n",
      " - Item: P2W2 (6/16) Bloomberg Ideathon (SubHeader)\n",
      " - Item: Homework (SubHeader)\n",
      " - Item: Homework: Watch Hackathon Video (Assignment)\n",
      " - Item: Homework: Upwardly Global Learning Paths: Tech Market/Resume/Cover Letter (Assignment)\n",
      " - Item: Homework: Draft Resume (Assignment)\n",
      " - Item: P2W2 NO CLASS MEETING 6/19 Juneteenth TKH Closed (SubHeader)\n",
      " - Item: P2W3 (6/26) Bloomberg Hackathon (SubHeader)\n",
      " - Item: Homework (SubHeader)\n",
      " - Item: Homework: Hackathon Activity Log + Judges' Feedback (Assignment)\n",
      " - Item: P2W4 (7/3) Resume + Digital Footprint (SubHeader)\n",
      " - Item: In Class Activity (SubHeader)\n",
      " - Item: In-Class Activity: Updated Resume (Assignment)\n",
      " - Item: Homework (SubHeader)\n",
      " - Item: Homework: Upwardly Global Learning Path: Marketing Yourself (Assignment)\n",
      " - Item: Homework: LinkedIn Profile (Assignment)\n",
      " - Item: Grades due by 7/7 [note for staff] (SubHeader)\n",
      " - Item:  P2W5 (7/10) Elevator Pitch + \"Tell Me About Yourself\" (SubHeader)\n",
      " - Item: Homework (SubHeader)\n",
      " - Item: Homework: Elevator Pitch Video (Assignment)\n",
      " - Item: Homework: Upwardly Global Learning Path: Networking (Assignment)\n",
      " - Item: P2W6 (7/17) Company Exposure (SubHeader)\n",
      " - Item: P2W7 (7/24) Career Exploration + Job Search (SubHeader)\n",
      " - Item: In Class Activity (SubHeader)\n",
      " - Item: In-Class Activity: Career Plan (Assignment)\n",
      " - Item: Homework (SubHeader)\n",
      " - Item: Homework: Weekly Job Applications & Networking (Due July 26) (Assignment)\n",
      " - Item: Prepare for next week's speed networking session. (SubHeader)\n",
      " - Item:  P2W8 (7/31) Speed Networking (SubHeader)\n",
      " - Item: Homework (SubHeader)\n",
      " - Item: (OPTIONAL) Homework: Weekly Job Applications & Networking (Due August 2) (Assignment)\n",
      " - Item: Homework: Upwardly Global Learning Path: Interview (Assignment)\n",
      " - Item: Grades due by 8/4 [note for staff] (SubHeader)\n",
      " - Item: P2W9 (8/7) SWOT + Confidence Gap (SubHeader)\n",
      " - Item: Career Class Option Selection (Due August 9) (Quiz)\n",
      " - Item: In Class Activity (SubHeader)\n",
      " - Item: Homework (SubHeader)\n",
      " - Item: P2W10 (8/14) Applications Questions + Behavioral Interviews (SubHeader)\n",
      " - Item: In Class Activity (SubHeader)\n",
      " - Item: In-Class Activity: Application Questions (Assignment)\n",
      " - Item: Homework (SubHeader)\n",
      " - Item: Homework: Option 1 - Weekly Job Applications & Progress Report (Due August 23) (Assignment)\n",
      " - Item: Prepare for mock behavioral interviews that will take place next week. (SubHeader)\n",
      " - Item: P2W11 (8/21) Behavioral Mock Interviews (SubHeader)\n",
      " - Item: Homework (SubHeader)\n",
      " - Item: Homework: Option 1 - Weekly Job Applications & Progress Report (Due August 30) (Assignment)\n",
      " - Item: Homework: Option 2 - Career Development Report (Due August 30) (Assignment)\n",
      " - Item: P2W12 (8/28) Portfolio Storytelling (Last Week of Phase 2) (SubHeader)\n",
      " - Item: Homework (SubHeader)\n",
      " - Item: Homework: Portfolio (Assignment)\n",
      " - Item: Grades due by 9/2 [note for staff] (SubHeader)\n",
      "  Module_id: 1222\n",
      "  Module: Monthly Program Satisfaction Surveys\n",
      " - Item: March Program Satisfaction (Assignment)\n",
      " - Item: April Program Satisfaction  (Assignment)\n",
      " - Item: May Program Satisfaction (Assignment)\n",
      " - Item: June Program Satisfaction  (Assignment)\n",
      " - Item: July Program Satisfaction (Assignment)\n",
      " - Item: August Program Satisfaction  (Assignment)\n",
      " - Item: September Program Satisfaction (Assignment)\n",
      " - Item: October Program Satisfaction  (Assignment)\n",
      " - Item: November Program Satisfaction  (Assignment)\n"
     ]
    }
   ],
   "source": [
    "# Pulling modules from courses on Canvas \n",
    "modules = course.get_modules()\n",
    "\n",
    "for module in modules:\n",
    "    print(f\"  Module_id: {module.id}\")\n",
    "    print(f\"  Module: {module.name}\")\n",
    "    module_items = module.get_module_items()\n",
    "    for item in module_items:\n",
    "        print(f\" - Item: {item.title} ({item.type})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd6497b",
   "metadata": {},
   "source": [
    "## Embedding Tokenized Canvas modules (Texts/items). Then Turning Those Embddings into a facts list + FAISS index that we can query.\n",
    "\n",
    "1. Build facts (+ metadata) from Canvas\n",
    "  - This pulls Pages' text (HTML â†’ plain text), and adds light facts for  \n",
    "    External URLs / Files / SubHeaders â†’ we may have to extend this later.\n",
    "\n",
    "## single FAISS store:\n",
    "- Simple/Demo MVP, which tags every fact with a domain and use a tiny auto-router\n",
    "  - Two-way short route descriptions (technical.index and career.index)\n",
    "    - Pulls multiple Canvas courses\n",
    "    - Builds one facts/metas list with domain in metadata\n",
    "    - And creates one FAISS index\n",
    "- This method routes queries to technical / career / all--automatically and filters hits accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d73e785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-course to ONE FAISS store + simple router using career and technical courses\n",
    "\n",
    "# 0) CONFIG: map course names to domain buckets\n",
    "\n",
    "DOMAINS = {\n",
    "    \"technical\": [\n",
    "        \"Foundations '25 Data Science\",\n",
    "        \"Foundations Course\",\n",
    "        \"IF '25 Data Science Cohort A\",\n",
    "    ],\n",
    "    \"career\": [\n",
    "        \"IF '25 NY Career Readiness and Success\",\n",
    "    ],\n",
    "}\n",
    "# Short route descriptions--We can add more if needed (used for auto routing purpos)\n",
    "ROUTE_DESC = {\n",
    "    \"technical\": \"Technical class content: Python, SQL, statistics, machine learning, slides, labs, code, algorithms, data science, lecture notes.\",\n",
    "    \"career\":    \"Career readiness content: resumes, cover letters, job search, interviews, career prep, LinkedIn, networking, internship resources.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4da07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Utility: HTML â†’ text, light chunking\n",
    "def strip_html(html: str) -> str:\n",
    "    \"\"\"Removes HTML tags and returns a single string.\n",
    "\n",
    "    This function parses an HTML string, extracts the text, and joins\n",
    "    it into a single string. It handles whitespace by\n",
    "    replacing multiple spaces with a single space and stripping leading/\n",
    "    trailing whitespace.\n",
    "\n",
    "    Args:\n",
    "        html (str): The HTML content to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: The plain-text content without HTML tags.\n",
    "    \"\"\"\n",
    "    if not html: return \"\"\n",
    "    txt = \" \".join(BeautifulSoup(html, \"html.parser\").stripped_strings)\n",
    "    return re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "\n",
    "def chunk_text(text, max_chars=600):\n",
    "    \"\"\"Splits a long string into a list of smaller text chunks.\n",
    "\n",
    "    This function chunks a text string based on a maximum character\n",
    "    limit. It attempts to split the text at natural breaks, such as\n",
    "    newlines or the end of a sentence (followed by a period and a space),\n",
    "    to create more readable chunks.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be chunked.\n",
    "        max_chars (int, optional): The maximum number of characters\n",
    "            for each chunk. Defaults to 600.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of text chunks, where each chunk is\n",
    "            no longer than `max_chars`.\n",
    "    \"\"\"\n",
    "    if not text: return []\n",
    "    parts = re.split(r\"(\\n|\\.\\s+)\", text)\n",
    "    buf, chunks = \"\", []\n",
    "    for p in parts:\n",
    "        buf += p\n",
    "        if len(buf) >= max_chars:\n",
    "            chunks.append(buf.strip()); buf = \"\"\n",
    "    if buf.strip(): chunks.append(buf.strip())\n",
    "    return [c for c in chunks if c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1eb3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected courses: [\"Foundations '25 Data Science\", 'Foundations Course', \"IF '25 Data Science Cohort A\", \"IF '25 NY Career Readiness and Success\"]\n"
     ]
    }
   ],
   "source": [
    "# 2) Select courses by name (use your Canvas client `canvas`)\n",
    "\n",
    "def course_domain(course_name: str):\n",
    "    \"\"\"\n",
    "    Finds the index of the most recent entry in a DataFrame.\n",
    "    It returns the integer index\n",
    "    of this last row.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The pandas DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "        int: The integer index of the most recent entry.\n",
    "    \"\"\"\n",
    "    for dom, names in DOMAINS.items():\n",
    "        if any(course_name.startswith(n) for n in names):\n",
    "            return dom\n",
    "    return \"other\"\n",
    "\n",
    "wanted_prefixes = sum(DOMAINS.values(), [])\n",
    "all_courses = [c for c in canvas.get_courses(enrollment_state=\"active\")\n",
    "               if any(c.name.startswith(p) for p in wanted_prefixes)]\n",
    "\n",
    "print(\"Selected courses:\", [c.name for c in all_courses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a7834b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course: Foundations '25 Data Science (domain=technical)\n",
      "Course: Foundations Course (domain=technical)\n",
      "Course: IF '25 Data Science Cohort A (domain=technical)\n",
      "Course: IF '25 NY Career Readiness and Success (domain=career)\n",
      "Built 324 facts\n"
     ]
    }
   ],
   "source": [
    "# 3) Build facts + metas from ALL selected courses\n",
    "facts, metas = [], []\n",
    "for crs in all_courses:\n",
    "    dom = course_domain(crs.name)\n",
    "    print(f\"Course: {crs.name} (domain={dom})\")\n",
    "    for module in crs.get_modules():\n",
    "        for item in module.get_module_items():\n",
    "            t = (item.type or \"\").strip()\n",
    "            if t == \"Page\":\n",
    "                page = crs.get_page(item.page_url)\n",
    "                text = strip_html(getattr(page, \"body\", \"\"))\n",
    "                for chunk in chunk_text(text, max_chars=600):\n",
    "                    facts.append(f\"[{dom}] {crs.name} > {module.name} > {item.title}: {chunk}\")\n",
    "                    metas.append({\n",
    "                        \"domain\": dom,\n",
    "                        \"course_id\": crs.id, \"course_name\": crs.name,\n",
    "                        \"module_id\": module.id, \"module_name\": module.name,\n",
    "                        \"item_title\": item.title, \"type\": \"Page\",\n",
    "                        \"url\": getattr(page, \"html_url\", None)\n",
    "                    })\n",
    "            elif t in (\"ExternalUrl\", \"ExternalTool\"):\n",
    "                facts.append(f\"[{dom}] {crs.name} > {module.name} > {item.title}: external link {getattr(item, 'external_url', '')}\")\n",
    "                metas.append({\n",
    "                    \"domain\": dom, \"course_id\": crs.id, \"course_name\": crs.name,\n",
    "                    \"module_id\": module.id, \"module_name\": module.name,\n",
    "                    \"item_title\": item.title, \"type\": t,\n",
    "                    \"url\": getattr(item, \"external_url\", None)\n",
    "                })\n",
    "            elif t == \"File\":\n",
    "                facts.append(f\"[{dom}] {crs.name} > {module.name} > {item.title} (file)\")\n",
    "                metas.append({\n",
    "                    \"domain\": dom, \"course_id\": crs.id, \"course_name\": crs.name,\n",
    "                    \"module_id\": module.id, \"module_name\": module.name,\n",
    "                    \"item_title\": item.title, \"type\": \"File\", \"file_id\": item.content_id\n",
    "                })\n",
    "            elif t == \"SubHeader\":\n",
    "                continue\n",
    "            else:\n",
    "                facts.append(f\"[{dom}] {crs.name} > {module.name} > {item.title} ({t})\")\n",
    "                metas.append({\n",
    "                    \"domain\": dom, \"course_id\": crs.id, \"course_name\": crs.name,\n",
    "                    \"module_id\": module.id, \"module_name\": module.name,\n",
    "                    \"item_title\": item.title, \"type\": t\n",
    "                })\n",
    "\n",
    "print(f\"Built {len(facts)} facts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3952bff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0692eebf1f14bd7a055bc54702e99ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS ntotal: 324\n"
     ]
    }
   ],
   "source": [
    "# 4) Embed â€” use GPU if available, else CPU\n",
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=DEVICE)\n",
    "print(\"model device:\", model.device)\n",
    "\n",
    "# (optional) quick warm-up on GPU\n",
    "if DEVICE == \"cuda\":\n",
    "    _ = model.encode([\"warm up\"], show_progress_bar=False)\n",
    "\n",
    "# pick a sensible batch size per device\n",
    "BATCH = 192 if DEVICE == \"cuda\" else 64\n",
    "\n",
    "emb = model.encode(\n",
    "    facts,\n",
    "    batch_size=BATCH,\n",
    "    normalize_embeddings=True,   # cosine-ready\n",
    "    convert_to_numpy=True,       # returns NumPy on CPU for FAISS\n",
    "    show_progress_bar=True\n",
    ").astype(\"float32\")\n",
    "\n",
    "d = emb.shape[1]\n",
    "index = faiss.IndexFlatIP(d)               # cosine (vectors normalized)\n",
    "index.add(emb)\n",
    "print(\"FAISS ntotal:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6896266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Router: choose technical / career / all based on similarity to route descriptions\n",
    "route_emb = {k: model.encode([v], normalize_embeddings=True).astype(\"float32\") for k,v in ROUTE_DESC.items()}\n",
    "\n",
    "def choose_scope(query, margin=0.05):\n",
    "    \"\"\"\n",
    "    Selects the most relevant content category for a given user query.\n",
    "\n",
    "    This function compares a user's question or search query to predefined\n",
    "    categories (like \"technical\" or \"career\") to find the best match. It\n",
    "    uses the \"sentence transformer\" to understand\n",
    "    the meaning of the words.\n",
    "\n",
    "    The function returns the best-matching category. However, if two\n",
    "    categories are very close in relevance (within a small \"margin\"), it\n",
    "    chooses \"all\" to be safe, meaning the query could apply to both.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's question or search query.\n",
    "        margin (float, optional): A small number that decides how close\n",
    "                                  two categories can be before the function\n",
    "                                  chooses \"all\". Defaults to 0.05.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, Dict[str, float]]: A tuple containing:\n",
    "            - The name of the best-matching category (e.g., \"technical\" or \"all\").\n",
    "            - A dictionary showing the similarity score for each category.\n",
    "    \"\"\"\n",
    "    q = model.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
    "    sims = {k: float((q @ route_emb[k].T)[0,0]) for k in ROUTE_DESC}\n",
    "    best = max(sims, key=sims.get)\n",
    "    # if not clearly better, use 'all'\n",
    "    ordered = sorted(sims.items(), key=lambda x: x[1], reverse=True)\n",
    "    if ordered[0][1] - ordered[1][1] < margin:\n",
    "        return \"all\", sims\n",
    "    return best, sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca57c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Search with optional scope filter (auto by default)\n",
    "def search(query, k=5, scope=\"auto\"):\n",
    "    \"\"\"Determines the most relevant content scope for a given query.\n",
    "\n",
    "    This function uses semantic similarity to compare a user's query against\n",
    "    predefined scope descriptions. It selects the best-matching scope (\n",
    "    \"technical\" or \"career\"). If the top two scores are within the specified\n",
    "    margin of each other, it returns \"all\" to indicate ambiguity.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's input query string.\n",
    "        margin (float, optional): The similarity threshold for determining\n",
    "                                  ambiguity between scopes. Defaults to 0.05.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, Dict[str, float]]: A tuple containing:\n",
    "            - The selected scope string (\"technical\", \"career\", or \"all\").\n",
    "            - A dictionary of similarity scores for each scope.\n",
    "    \"\"\"\n",
    "    if scope == \"auto\":\n",
    "        scope, sims = choose_scope(query)\n",
    "    q = model.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
    "    # pull more then filter by domain\n",
    "    D, I = index.search(q, k*8)\n",
    "    hits = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        if idx == -1: continue\n",
    "        m = metas[idx]\n",
    "        if scope != \"all\" and m[\"domain\"] != scope:\n",
    "            continue\n",
    "        hits.append({\"score\": float(score), \"fact\": facts[idx], \"meta\": m})\n",
    "        if len(hits) >= k: break\n",
    "    # if not enough in-scope, backfill with any\n",
    "    if len(hits) < k:\n",
    "        for score, idx in zip(D[0], I[0]):\n",
    "            if idx == -1: continue\n",
    "            if any(h[\"meta\"] is metas[idx] for h in hits): continue\n",
    "            hits.append({\"score\": float(score), \"fact\": facts[idx], \"meta\": metas[idx]})\n",
    "            if len(hits) >= k: break\n",
    "    return scope, hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d818042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: Where did we define precision vs. recall?   [scope=technical]\n",
      "  0.382 :: IF '25 Data Science Cohort A > P2W3 (6/23-6/27) Classification Algorithms > ðŸ’» W3D2 (6/24) Logistic Regression Accuracy Metrics (Page)  [https://tkh.instructure.com/courses/172/pages/w3d2-6-slash-24-logistic-regression-accuracy-metrics]\n",
      "  0.306 :: Foundations '25 Data Science > Week 5:  Statistics(Feb. 24th- Feb. 27th) > What is Data Science? (Page)  [https://tkh.instructure.com/courses/165/pages/what-is-data-science]\n",
      "  0.276 :: IF '25 Data Science Cohort A > P2W11 (8/18-8/22) Agents & End of Phase Project > ðŸ’» W11D1 (8/18) Applied LLM Review & AI Agents (Page)  [https://tkh.instructure.com/courses/172/pages/w11d1-8-slash-18-applied-llm-review-and-ai-agents]\n",
      "  0.263 :: IF '25 Data Science Cohort A > P2W9 (8/4-8/8) NLP Foundations & Transformers > ðŸ“š P2W9 Overview & Lesson Plan (Page)  [https://tkh.instructure.com/courses/172/pages/p2w9-overview-and-lesson-plan]\n",
      "\n",
      "Q: tips for a resume and cover letter?   [scope=career]\n",
      "  0.443 :: IF '25 NY Career Readiness and Success > Phase 2 (6/9-8/29) > Homework: Upwardly Global Learning Paths: Tech Market/Resume/Cover Letter (Assignment)\n",
      "  0.426 :: IF '25 NY Career Readiness and Success > Phase 2 (6/9-8/29) > Homework: Draft Resume (Assignment)\n",
      "  0.368 :: IF '25 NY Career Readiness and Success > Phase 2 (6/9-8/29) > Homework: LinkedIn Profile (Assignment)\n",
      "  0.354 :: IF '25 NY Career Readiness and Success > Phase 2 (6/9-8/29) > Homework: Upwardly Global Learning Path: Marketing Yourself (Assignment)\n",
      "\n",
      "Q: What lecture slides did we learn about control flow?   [scope=technical]\n",
      "  0.389 :: Foundations Course > Week 1: Foundations For Success (Jan. 27th-Jan. 30th) > W1D2: Learning How to Learn (Page)  [https://tkh.instructure.com/courses/162/pages/w1d2-learning-how-to-learn]\n",
      "  0.350 :: Foundations Course > Week 2: Charting Your Path (Feb. 3rd- Feb 6th) > W2D3: Track Exploration (Track Mini LessonsÂ Continued) (Page)  [https://tkh.instructure.com/courses/162/pages/w2d3-track-exploration-track-mini-lessons-continued]\n",
      "  0.348 :: Foundations Course > Week 3: Tech Essentials (Feb. 10th- Feb 13th) > Week 3 Overview  (Page)  [https://tkh.instructure.com/courses/162/pages/week-3-overview]\n",
      "  0.339 :: Foundations Course > Week 2: Charting Your Path (Feb. 3rd- Feb 6th) > W2D2: Track Exploration (Track Mini Lessons)Â  (Page)  [https://tkh.instructure.com/courses/162/pages/w2d2-track-exploration-track-mini-lessons]\n"
     ]
    }
   ],
   "source": [
    "# 7) Try it out with some pre-test test-prompts\n",
    "tests = [\n",
    "    \"Where did we define precision vs. recall?\",\n",
    "    \"tips for a resume and cover letter?\",\n",
    "    \"What lecture slides did we learn about control flow?\",\n",
    "  ]\n",
    "for q in tests:\n",
    "    scope, hits = search(q, k=4, scope=\"auto\")\n",
    "    print(f\"\\nQ: {q}   [scope={scope}]\")\n",
    "    if not hits: print(\"  (no hits)\"); continue\n",
    "    for h in hits:\n",
    "        m = h[\"meta\"]\n",
    "        cite = f\"{m['course_name']} > {m['module_name']} > {m['item_title']} ({m['type']})\"\n",
    "        if m.get(\"url\"): cite += f\"  [{m['url']}]\"\n",
    "        print(f\"  {h['score']:.3f} :: {cite}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffaf296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: C:\\Users\\MINA-G\\SlideHunter\\data\\faiss\\canvas.index\n",
      "saved: C:\\Users\\MINA-G\\SlideHunter\\data\\faiss\\facts.json\n"
     ]
    }
   ],
   "source": [
    "# --- Persist FAISS + metadata to the repo root (SLIDEHUNTER/) ---\n",
    "from pathlib import Path\n",
    "import os, json, faiss\n",
    "\n",
    "# 0) Resolve project base: prefer env var; else step out of notebooks/\n",
    "ENV_BASE = os.getenv(\"SLIDEHUNTER_BASE\") or os.getenv(\"SLIDEHUNT_BASE\")\n",
    "if ENV_BASE:\n",
    "    BASE = Path(ENV_BASE).resolve()\n",
    "else:\n",
    "    here = Path.cwd().resolve()\n",
    "    BASE = here.parent if here.name.lower() == \"notebooks\" else here  # run from repo root if you're inside notebooks/\n",
    "\n",
    "# 1) Paths under the repo\n",
    "STORE_DIR  = BASE / \"data\" / \"faiss\"\n",
    "INDEX_PATH = STORE_DIR / \"canvas.index\"\n",
    "FACTS_PATH = STORE_DIR / \"facts.json\"\n",
    "STORE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2) Save / Load helpers\n",
    "def save_store(index, facts, metas, index_path=INDEX_PATH, facts_path=FACTS_PATH):\n",
    "    \"\"\"Saves the FAISS index, facts, and metadata.\n",
    "\n",
    "    This function serializes and saves the vector search index and its\n",
    "    corresponding data. The FAISS index is saved in its native binary format,\n",
    "    while the facts and their metadata are stored together in a JSON file.\n",
    "\n",
    "    Args:\n",
    "        index (faiss.Index): The FAISS index to be saved.\n",
    "        facts (List[str]): A list of text strings corresponding to the facts.\n",
    "        metas (List[Dict]): A list of dictionaries containing metadata for\n",
    "                            each fact.\n",
    "        index_path (Path, optional): The file path to save the FAISS index.\n",
    "                                     Defaults to INDEX_PATH.\n",
    "        facts_path (Path, optional): The file path to save the facts and\n",
    "                                     metadata. Defaults to FACTS_PATH.\n",
    "    \"\"\"\n",
    "    faiss.write_index(index, str(index_path))\n",
    "    with open(facts_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"facts\": facts, \"metas\": metas}, f, ensure_ascii=False)\n",
    "    print(\"saved:\", index_path)\n",
    "    print(\"saved:\", facts_path)\n",
    "\n",
    "def load_store(index_path=INDEX_PATH, facts_path=FACTS_PATH):\n",
    "    \"\"\"Loads your saved search data from your computer.\n",
    "\n",
    "    This function opens the files that were saved by `save_store` and brings\n",
    "    your FAISS index, facts, and metadata back into the program.\n",
    "    This is much faster than having to create all the data again.\n",
    "\n",
    "    Args:\n",
    "        index_path (Path, optional): The location of the saved index file.\n",
    "        facts_path (Path, optional): The location of the saved facts file.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[faiss.Index, List[str], List[Dict]]: The loaded index, facts,\n",
    "                                                     and metadata.\n",
    "    \"\"\"\n",
    "    idx = faiss.read_index(str(index_path))\n",
    "    data = json.load(open(facts_path, \"r\", encoding=\"utf-8\"))\n",
    "    print(\"loaded:\", index_path, \"and\", facts_path)\n",
    "    return idx, data[\"facts\"], data[\"metas\"]\n",
    "\n",
    "# 3) Save right after you build `index`, `facts`, `metas`\n",
    "save_store(index, facts, metas)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

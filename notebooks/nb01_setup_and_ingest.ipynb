{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81cf5caf",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ccc; border-radius: 12px; padding: 20px; max-width: 550px; margin: auto; background-color: #1e1e1e; color: #f0f0f0; font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "\n",
    "  <div style=\"text-align: center; margin-bottom: 20px;\">\n",
    "    <img src=\"..\\assets\\images\\SlideHunter_Logo.png\" \n",
    "         alt=\"SlideHunter Mockup Logo\"\n",
    "         style=\"width: 60%; max-width: 60%; height: auto; border-radius: 8px; box-shadow: 0 0 10px rgba(0,0,0,0.4);\">\n",
    "  </div>\n",
    "\n",
    "  <blockquote style=\"margin: 0; padding: 10px 20px; border-left: 4px solid #4faaff;\">\n",
    "    <p><strong>\n",
    "      SlideHunter App Powered by AI\n",
    "    </strong></p>\n",
    "    <p>\n",
    "     User Interface (UI) : \n",
    "      <a href=\"..\\assets\\images\\SlideHunter_Logo.png\" target=\"_blank\" style=\"color: #4faaff;\">\n",
    "        Find exactly where a topic was covered in course materials. Fast answers with precise slide/page citations.\n",
    "      </a>\n",
    "    </p>\n",
    "  </blockquote>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95821ea5",
   "metadata": {},
   "source": [
    "## Check if GPU is alalable for faster embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ce45304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda_available: False\n",
      "torch.cuda: None\n",
      "device: CPU only\n"
     ]
    }
   ],
   "source": [
    "import torch, platform\n",
    "\n",
    "print(\"cuda_available:\", torch.cuda.is_available())\n",
    "print(\"torch.cuda:\", torch.version.cuda)          # None if CUDA isn't available\n",
    "print(\"device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU only\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6bfa35",
   "metadata": {},
   "source": [
    "# Setup & Ingest Notebook\n",
    "\n",
    "## Retrieval-Augmented-Generation (RAG)\n",
    "\n",
    "Retrieval-Augmented Generation is a *technique* to improve an LLM's responses by:\n",
    "* Retrieving relevant documents from a knowledge store (such FAISS or a vector DB).\n",
    "* Augmenting the model's prompt with those documents.\n",
    "* Generating an answer using the model with this extra context.\n",
    "\n",
    "With RAG, the model is handed the right documents at generation time. That is, the model does not respond to a user's queries until it refers to a specified set of documents.\n",
    "\n",
    "The FAISS store acts as our long-term memory for domain knowledge. The RAG pipeline serves as the reasoning loop: it retrieves the most relevant content, assembles the top results into a compact context, and passes that context to the LLM to produce a grounded, citeable response.\n",
    "- Think of FAISS as the library and RAG as the librarian: FAISS shelves all the course knowledge; RAG finds the right books, marks the key pages, and hands them to the model to explain.\n",
    "\n",
    "This notebook parses Canvas Learning Management System (LMS) (cours-level), embeds chunks, and builds a FAISS Store \n",
    "\n",
    "\n",
    "## Install (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04fbd047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning surpresser\n",
    "import os\n",
    "\n",
    "# Tell Hugging Face to skip TensorFlow/Flax so they never import TensorFlow (TF).\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "\n",
    "# Quiet TF logs if something still pulls it in.\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # 1=INFO, 2=WARNING, 3=ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c9353a",
   "metadata": {},
   "source": [
    "## Imports and Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "959f56d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL If env is missing packages, or in a fresh/new environment\n",
    "#%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53230a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json, os\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np, faiss, re, json, os\n",
    "from canvasapi import Canvas\n",
    "import torch\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c2be23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "config = dotenv_values() # load .env file\n",
    "\n",
    "# Injest and process data from Canvas Sections\n",
    "# Set up Canva API client\n",
    "CANVAS_BASE_URL = config.get(\"CANVAS_BASE_URL\")\n",
    "CANVAS_TOKEN=config.get(\"CANVAS_TOKEN\")\n",
    "# Initialize Canvas API client \n",
    "canvas = Canvas(CANVAS_BASE_URL, CANVAS_TOKEN)\n",
    "\n",
    "# Pass in OpenAI API Key fron .evn var/file\n",
    "OPENAI_API_KEY = config.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad90c594",
   "metadata": {},
   "source": [
    "## Injest and process data from Canvas Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "963e5605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of courses\n",
    "my_courses = canvas.get_courses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a85877e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foundations '25 Data Science\n",
      "Foundations Course\n",
      "IF '25 Data Science Cohort A\n",
      "IF '25 NY Career Readiness and Success\n"
     ]
    }
   ],
   "source": [
    "# Pulling courses from Canvas and save to'course_list[]'\n",
    "my_courses = canvas.get_courses()\n",
    "course_list = []\n",
    "\n",
    "for course in my_courses:\n",
    "    print(course.name)\n",
    "    course_list.append(course)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0df8d2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Module_id: 1118\n",
      "  Module: Fellow Resources\n",
      " - Item: Fellow Success Resources (Page)\n",
      "  Module_id: 1239\n",
      "  Module: Phase 2 (6/9-8/29)\n",
      " - Item: 1:1 with Career Instructor (August) (Assignment)\n",
      " - Item: Homework: Option 1 - Weekly Job Applications & Progress Report (Due August 30) (Assignment)\n",
      " - Item: P2W1 (6/12) NO CAREER CLASS - TECHNICAL CLASS (SubHeader)\n",
      " - Item: P2W2 (6/16) Bloomberg Ideathon (SubHeader)\n",
      " - Item: Homework (SubHeader)\n",
      " - Item: Homework: Watch Hackathon Video (Assignment)\n",
      " - Item: Homework: Upwardly Global Learning Paths: Tech Market/Resume/Cover Letter (Assignment)\n",
      " - Item: Homework: Draft Resume (Assignment)\n",
      " - Item: P2W2 NO CLASS MEETING 6/19 Juneteenth TKH Closed (SubHeader)\n",
      " - Item: P2W3 (6/26) Bloomberg Hackathon (SubHeader)\n",
      " - Item: Homework (SubHeader)\n",
      " - Item: Homework: Hackathon Activity Log + Judges' Feedback (Assignment)\n",
      " - Item: P2W4 (7/3) Resume + Digital Footprint (SubHeader)\n",
      " - Item: In Class Activity (SubHeader)\n",
      " - Item: In-Class Activity: Updated Resume (Assignment)\n",
      " - Item: Homework (SubHeader)\n",
      " - Item: Homework: Upwardly Global Learning Path: Marketing Yourself (Assignment)\n",
      " - Item: Homework: LinkedIn Profile (Assignment)\n",
      " - Item: Grades due by 7/7 [note for staff] (SubHeader)\n",
      " - Item:  P2W5 (7/10) Elevator Pitch + \"Tell Me About Yourself\" (SubHeader)\n",
      " - Item: Homework (SubHeader)\n",
      " - Item: Homework: Elevator Pitch Video (Assignment)\n",
      " - Item: Homework: Upwardly Global Learning Path: Networking (Assignment)\n",
      " - Item: P2W6 (7/17) Company Exposure (SubHeader)\n",
      " - Item: P2W7 (7/24) Career Exploration + Job Search (SubHeader)\n",
      " - Item: In Class Activity (SubHeader)\n",
      " - Item: In-Class Activity: Career Plan (Assignment)\n",
      " - Item: Homework (SubHeader)\n",
      " - Item: Homework: Weekly Job Applications & Networking (Due July 26) (Assignment)\n",
      " - Item: Prepare for next week's speed networking session. (SubHeader)\n",
      " - Item:  P2W8 (7/31) Speed Networking (SubHeader)\n",
      " - Item: Homework (SubHeader)\n",
      " - Item: (OPTIONAL) Homework: Weekly Job Applications & Networking (Due August 2) (Assignment)\n",
      " - Item: Homework: Upwardly Global Learning Path: Interview (Assignment)\n",
      " - Item: Grades due by 8/4 [note for staff] (SubHeader)\n",
      " - Item: P2W9 (8/7) SWOT + Confidence Gap (SubHeader)\n",
      " - Item: In Class Activity (SubHeader)\n",
      " - Item: Homework (SubHeader)\n",
      " - Item: P2W10 (8/14) Applications Questions + Behavioral Interviews (SubHeader)\n",
      " - Item: In Class Activity (SubHeader)\n",
      " - Item: In-Class Activity: Application Questions (Assignment)\n",
      " - Item: Homework (SubHeader)\n",
      " - Item: Homework: Option 1 - Weekly Job Applications & Progress Report (Due August 23) (Assignment)\n",
      " - Item: Prepare for mock behavioral interviews that will take place next week. (SubHeader)\n",
      " - Item: P2W11 (8/21) Behavioral Mock Interviews (SubHeader)\n",
      " - Item: Homework (SubHeader)\n",
      " - Item: Homework: Option 1 - Weekly Job Applications & Progress Report (Due August 30) (Assignment)\n",
      " - Item: Homework: Option 2 - Career Development Report (Due August 30) (Assignment)\n",
      " - Item: P2W12 (8/28) Portfolio Storytelling (Last Week of Phase 2) (SubHeader)\n",
      " - Item: Homework (SubHeader)\n",
      " - Item: Homework: Portfolio (Assignment)\n",
      " - Item: Grades due by 9/2 [note for staff] (SubHeader)\n",
      "  Module_id: 1222\n",
      "  Module: Monthly Program Satisfaction Surveys\n",
      " - Item: March Program Satisfaction (Assignment)\n",
      " - Item: April Program Satisfaction  (Assignment)\n",
      " - Item: May Program Satisfaction (Assignment)\n",
      " - Item: June Program Satisfaction  (Assignment)\n",
      " - Item: July Program Satisfaction (Assignment)\n",
      " - Item: August Program Satisfaction  (Assignment)\n",
      " - Item: September Program Satisfaction (Assignment)\n",
      " - Item: October Program Satisfaction  (Assignment)\n",
      " - Item: November Program Satisfaction  (Assignment)\n"
     ]
    }
   ],
   "source": [
    "# Pull module from course on Canvas   \n",
    "modules = course.get_modules()\n",
    "\n",
    "# List modules and there items\n",
    "for module in modules:\n",
    "    print(f\"  Module_id: {module.id}\")\n",
    "    print(f\"  Module: {module.name}\")\n",
    "    module_items = module.get_module_items()\n",
    "    for item in module_items:\n",
    "        print(f\" - Item: {item.title} ({item.type})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd6497b",
   "metadata": {},
   "source": [
    "## Embedding Tokenized Canvas modules (Texts/items). Then Turning Those Embddings into a facts list + FAISS index that we can query.\n",
    "\n",
    "1. Build facts (+ metadata) from Canvas\n",
    "  - This pulls Pages' text (HTML → plain text), and adds light facts for  \n",
    "    External URLs / Files / SubHeaders.\n",
    "\n",
    "## single FAISS store:\n",
    "- Simple/Demo MVP, which tags every fact with a domain and use a tiny auto-router\n",
    "  - Two-way short route descriptions (technical.index and career.index)\n",
    "    - Pulls multiple Canvas courses\n",
    "    - Builds one facts/metas list with domain in metadata\n",
    "    - And creates one FAISS index\n",
    "- This method routes queries to technical / career / all--automatically and filters hits accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d73e785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-course to ONE FAISS store + simple router using career and technical courses\n",
    "# CONFIG: map course names to domain buckets\n",
    "DOMAINS = {\n",
    "    \"technical\": [\n",
    "        \"Foundations '25 Data Science\",\n",
    "        \"Foundations Course\",\n",
    "        \"IF '25 Data Science Cohort A\",\n",
    "    ],\n",
    "    \"career\": [\n",
    "        \"IF '25 NY Career Readiness and Success\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Short route descriptions--We can add more if needed (used for auto routing purposes)\n",
    "ROUTE_DESC = {\n",
    "    \"technical\": \"Technical class content: Python, SQL, statistics, machine learning, slides, labs, code, algorithms, data science, lecture notes.\",\n",
    "    \"career\":    \"Career readiness content: resumes, cover letters, job search, interviews, career prep, LinkedIn, networking, internship resources.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fb4da07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: HTML → text, light chunking\n",
    "def strip_html(html: str) -> str:\n",
    "    \"\"\"\n",
    "   This function strips HTML tags from a string and return clean text.\n",
    "    Args:\n",
    "        html (str): The HTML string to be cleaned.\n",
    "    Returns:\n",
    "        str: The cleaned text without HTML tags.\n",
    "    \"\"\"\n",
    "    if not html: return \"\"\n",
    "    txt = \" \".join(BeautifulSoup(html, \"html.parser\").stripped_strings)\n",
    "    return re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "\n",
    "def chunk_text(text, max_chars=600):\n",
    "    \"\"\"\n",
    "    This function splits text into chunks of max_chars, trying to split on sentence boundaries(. or \\n)\n",
    "    Args:\n",
    "        text (str): The text to be chunked.\n",
    "        max_chars (int): The maximum number of characters per chunk.\n",
    "    Returns:\n",
    "        List(str): A list of text chunks each with a maximum length of max_chars.\n",
    "    \"\"\"\n",
    "    if not text: return []\n",
    "    parts = re.split(r\"(\\n|\\.\\s+)\", text)\n",
    "    buf, chunks = \"\", []\n",
    "    for p in parts:\n",
    "        buf += p\n",
    "        if len(buf) >= max_chars:\n",
    "            chunks.append(buf.strip()); buf = \"\"\n",
    "    if buf.strip(): chunks.append(buf.strip())\n",
    "    return [c for c in chunks if c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c1eb3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected courses: [\"Foundations '25 Data Science\", 'Foundations Course', \"IF '25 Data Science Cohort A\", \"IF '25 NY Career Readiness and Success\"]\n"
     ]
    }
   ],
   "source": [
    "# Select courses by name (use your Canvas client `canvas`)\n",
    "\n",
    "def course_domain(course_name: str):\n",
    "    \"\"\" \n",
    "    This function maps a course name to a domain based on predefined prefixes.\n",
    "    Args:\n",
    "        course_name (str): The name of the course.\n",
    "    Returns:\n",
    "        str: The domain the course belongs to, or \"other\" if no match is found.\n",
    "    \"\"\"\n",
    "    for dom, names in DOMAINS.items():\n",
    "        if any(course_name.startswith(n) for n in names):\n",
    "            return dom\n",
    "    return \"other\"\n",
    "\n",
    "wanted_prefixes = sum(DOMAINS.values(), [])\n",
    "all_courses = [c for c in canvas.get_courses(enrollment_state=\"active\")\n",
    "               if any(c.name.startswith(p) for p in wanted_prefixes)]\n",
    "\n",
    "print(\"Selected courses:\", [c.name for c in all_courses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a7834b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course: Foundations '25 Data Science (domain=technical)\n",
      "Course: Foundations Course (domain=technical)\n",
      "Course: IF '25 Data Science Cohort A (domain=technical)\n",
      "Course: IF '25 NY Career Readiness and Success (domain=career)\n",
      "Built 324 facts\n"
     ]
    }
   ],
   "source": [
    "# Build a collection of \"facts\" (text snippets) and \"metas\" (metadata)\n",
    "# from all selected Canvas courses. These serve as the knowledge base\n",
    "# for downstream retrieval and search (BM25, FAISS, reranker, etc.).\n",
    "\n",
    "facts, metas = [], []\n",
    "\n",
    "# Iterate through every selected course\n",
    "for crs in all_courses:\n",
    "    # Determine domain classification for the course (e.g., \"technical\" or \"career\")\n",
    "    dom = course_domain(crs.name)\n",
    "    print(f\"Course: {crs.name} (domain={dom})\")\n",
    "\n",
    "    # Traverse all modules in the course\n",
    "    for module in crs.get_modules():\n",
    "        # Traverse all items (pages, files, links, etc.) in the module\n",
    "        for item in module.get_module_items():\n",
    "            t = (item.type or \"\").strip()  # Normalize item type string\n",
    "\n",
    "            if t == \"Page\":\n",
    "                # Handle regular content pages\n",
    "                page = crs.get_page(item.page_url)\n",
    "                text = strip_html(getattr(page, \"body\", \"\"))  # Clean HTML → plain text\n",
    "\n",
    "                # Chunk page text into ~600 char segments for indexing\n",
    "                for chunk in chunk_text(text, max_chars=600):\n",
    "                    # Store fact snippet (with hierarchical context)\n",
    "                    facts.append(f\"[{dom}] {crs.name} > {module.name} > {item.title}: {chunk}\")\n",
    "                    # Store associated metadata for later reference/citation\n",
    "                    metas.append({\n",
    "                        \"domain\": dom,\n",
    "                        \"course_id\": crs.id, \"course_name\": crs.name,\n",
    "                        \"module_id\": module.id, \"module_name\": module.name,\n",
    "                        \"item_title\": item.title, \"type\": \"Page\",\n",
    "                        \"url\": getattr(page, \"html_url\", None)\n",
    "                    })\n",
    "\n",
    "            elif t in (\"ExternalUrl\", \"ExternalTool\"):\n",
    "                # Handle external links/tools referenced in the module\n",
    "                facts.append(\n",
    "                    f\"[{dom}] {crs.name} > {module.name} > {item.title}: \"\n",
    "                    f\"external link {getattr(item, 'external_url', '')}\"\n",
    "                )\n",
    "                metas.append({\n",
    "                    \"domain\": dom,\n",
    "                    \"course_id\": crs.id, \"course_name\": crs.name,\n",
    "                    \"module_id\": module.id, \"module_name\": module.name,\n",
    "                    \"item_title\": item.title, \"type\": t,\n",
    "                    \"url\": getattr(item, \"external_url\", None)\n",
    "                })\n",
    "\n",
    "            elif t == \"File\":\n",
    "                # Handle uploaded files (e.g., PDFs, docs, slides)\n",
    "                facts.append(f\"[{dom}] {crs.name} > {module.name} > {item.title} (file)\")\n",
    "                metas.append({\n",
    "                    \"domain\": dom,\n",
    "                    \"course_id\": crs.id, \"course_name\": crs.name,\n",
    "                    \"module_id\": module.id, \"module_name\": module.name,\n",
    "                    \"item_title\": item.title, \"type\": \"File\",\n",
    "                    \"file_id\": item.content_id\n",
    "                })\n",
    "\n",
    "            elif t == \"SubHeader\":\n",
    "                # Skip non-content subheaders (used for visual grouping in Canvas)\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                # Handle any other miscellaneous item types\n",
    "                facts.append(f\"[{dom}] {crs.name} > {module.name} > {item.title} ({t})\")\n",
    "                metas.append({\n",
    "                    \"domain\": dom,\n",
    "                    \"course_id\": crs.id, \"course_name\": crs.name,\n",
    "                    \"module_id\": module.id, \"module_name\": module.name,\n",
    "                    \"item_title\": item.title, \"type\": t\n",
    "                })\n",
    "\n",
    "# Final status: number of fact snippets collected across all courses\n",
    "print(f\"Built {len(facts)} facts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3952bff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ead46ebdada49a18391720961f5e90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS ntotal: 324\n"
     ]
    }
   ],
   "source": [
    "# 4) Embed — use GPU if available, else CPU\n",
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=DEVICE)\n",
    "print(\"model device:\", model.device)\n",
    "\n",
    "# quick warm-up on GPU (if available)\n",
    "if DEVICE == \"cuda\":\n",
    "    _ = model.encode([\"warm up\"], show_progress_bar=False)\n",
    "\n",
    "# pick a sensible batch size per device\n",
    "BATCH = 192 if DEVICE == \"cuda\" else 64\n",
    "\n",
    "# Embed all facts (snippets) in batches\n",
    "# normalize for cosine similarity\n",
    "emb = model.encode(\n",
    "    facts,\n",
    "    batch_size=BATCH,\n",
    "    normalize_embeddings=True,   # cosine-ready\n",
    "    convert_to_numpy=True,       # returns NumPy on CPU for FAISS\n",
    "    show_progress_bar=True\n",
    ").astype(\"float32\")\n",
    "\n",
    "# Build a FAISS index (cosine similarity)\n",
    "d = emb.shape[1]\n",
    "index = faiss.IndexFlatIP(d)               # cosine (vectors normalized)\n",
    "index.add(emb)\n",
    "print(\"FAISS ntotal:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6896266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Router: choose technical / career / all based on similarity to route descriptions\n",
    "route_emb = {k: model.encode([v], normalize_embeddings=True).astype(\"float32\") for k,v in ROUTE_DESC.items()}\n",
    "\n",
    "def choose_scope(query, margin=0.05):\n",
    "    \"\"\"\n",
    "    This function chooses the best matching scope for a query based on similarity to predefined route descriptions.\n",
    "    Args: \n",
    "        query (str): The input query to be evaluated.\n",
    "        margin (float): The minimum difference in similarity scores to consider a clear best match.\n",
    "    Returns: \n",
    "        str: The best matching scope (\"technical\", \"career\", or \"all\").\n",
    "        dict: A dictionary of similarity scores for each scope.\n",
    "    \"\"\"\n",
    "    q = model.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
    "    sims = {k: float((q @ route_emb[k].T)[0,0]) for k in ROUTE_DESC}\n",
    "    best = max(sims, key=sims.get)\n",
    "    # if not clearly better, use 'all'\n",
    "    ordered = sorted(sims.items(), key=lambda x: x[1], reverse=True)\n",
    "    if ordered[0][1] - ordered[1][1] < margin:\n",
    "        return \"all\", sims\n",
    "    return best, sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca57c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Search with optional scope filter (auto by default)\n",
    "def search(query, k=5, scope=\"auto\"):\n",
    "    \"\"\"\n",
    "    This function searches the FAISS index for relevant facts based on the query and scope.\n",
    "    Args:\n",
    "        query (str): The input query to search for.\n",
    "        k (int): The number of top results to return.\n",
    "        scope (str): The scope to filter results by (\"technical\", \"career\", \"all\", or \"auto\" to choose automatically).\n",
    "    Returns:\n",
    "        str: The scope use for the search.\n",
    "        List[dict]: A list of top-k search results, each containing score, fact, and meta information.\n",
    "    \"\"\"\n",
    "    if scope == \"auto\":\n",
    "        scope, sims = choose_scope(query)\n",
    "    q = model.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
    "    # pull more then filter by domain\n",
    "    D, I = index.search(q, k*8)\n",
    "    hits = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        if idx == -1: continue\n",
    "        m = metas[idx]\n",
    "        if scope != \"all\" and m[\"domain\"] != scope:\n",
    "            continue\n",
    "        hits.append({\"score\": float(score), \"fact\": facts[idx], \"meta\": m})\n",
    "        if len(hits) >= k: break\n",
    "    # if not enough in-scope, backfill with any\n",
    "    if len(hits) < k:\n",
    "        for score, idx in zip(D[0], I[0]):\n",
    "            if idx == -1: continue\n",
    "            if any(h[\"meta\"] is metas[idx] for h in hits): continue\n",
    "            hits.append({\"score\": float(score), \"fact\": facts[idx], \"meta\": metas[idx]})\n",
    "            if len(hits) >= k: break\n",
    "    return scope, hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d818042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: Where did we define precision vs. recall?   [scope=technical]\n",
      "  0.382 :: IF '25 Data Science Cohort A > P2W3 (6/23-6/27) Classification Algorithms > 💻 W3D2 (6/24) Logistic Regression Accuracy Metrics (Page)  [https://tkh.instructure.com/courses/172/pages/w3d2-6-slash-24-logistic-regression-accuracy-metrics]\n",
      "  0.306 :: Foundations '25 Data Science > Week 5:  Statistics(Feb. 24th- Feb. 27th) > What is Data Science? (Page)  [https://tkh.instructure.com/courses/165/pages/what-is-data-science]\n",
      "  0.276 :: IF '25 Data Science Cohort A > P2W11 (8/18-8/22) Agents & End of Phase Project > 💻 W11D1 (8/18) Applied LLM Review & AI Agents (Page)  [https://tkh.instructure.com/courses/172/pages/w11d1-8-slash-18-applied-llm-review-and-ai-agents]\n",
      "  0.263 :: IF '25 Data Science Cohort A > P2W9 (8/4-8/8) NLP Foundations & Transformers > 📚 P2W9 Overview & Lesson Plan (Page)  [https://tkh.instructure.com/courses/172/pages/p2w9-overview-and-lesson-plan]\n",
      "\n",
      "Q: tips for a resume and cover letter?   [scope=career]\n",
      "  0.443 :: IF '25 NY Career Readiness and Success > Phase 2 (6/9-8/29) > Homework: Upwardly Global Learning Paths: Tech Market/Resume/Cover Letter (Assignment)\n",
      "  0.426 :: IF '25 NY Career Readiness and Success > Phase 2 (6/9-8/29) > Homework: Draft Resume (Assignment)\n",
      "  0.368 :: IF '25 NY Career Readiness and Success > Phase 2 (6/9-8/29) > Homework: LinkedIn Profile (Assignment)\n",
      "  0.354 :: IF '25 NY Career Readiness and Success > Phase 2 (6/9-8/29) > Homework: Upwardly Global Learning Path: Marketing Yourself (Assignment)\n",
      "\n",
      "Q: What lecture slides did we learn about control flow?   [scope=technical]\n",
      "  0.389 :: Foundations Course > Week 1: Foundations For Success (Jan. 27th-Jan. 30th) > W1D2: Learning How to Learn (Page)  [https://tkh.instructure.com/courses/162/pages/w1d2-learning-how-to-learn]\n",
      "  0.350 :: Foundations Course > Week 2: Charting Your Path (Feb. 3rd- Feb 6th) > W2D3: Track Exploration (Track Mini Lessons Continued) (Page)  [https://tkh.instructure.com/courses/162/pages/w2d3-track-exploration-track-mini-lessons-continued]\n",
      "  0.348 :: Foundations Course > Week 3: Tech Essentials (Feb. 10th- Feb 13th) > Week 3 Overview  (Page)  [https://tkh.instructure.com/courses/162/pages/week-3-overview]\n",
      "  0.339 :: Foundations Course > Week 2: Charting Your Path (Feb. 3rd- Feb 6th) > W2D2: Track Exploration (Track Mini Lessons)  (Page)  [https://tkh.instructure.com/courses/162/pages/w2d2-track-exploration-track-mini-lessons]\n"
     ]
    }
   ],
   "source": [
    "# Try it out with some pre-test test-prompts\n",
    "tests = [\n",
    "    \"Where did we define precision vs. recall?\",\n",
    "    \"tips for a resume and cover letter?\",\n",
    "    \"What lecture slides did we learn about control flow?\",\n",
    "  ]\n",
    "for q in tests:\n",
    "    scope, hits = search(q, k=4, scope=\"auto\")\n",
    "    print(f\"\\nQ: {q}   [scope={scope}]\")\n",
    "    if not hits: print(\"  (no hits)\"); continue\n",
    "    for h in hits:\n",
    "        m = h[\"meta\"]\n",
    "        cite = f\"{m['course_name']} > {m['module_name']} > {m['item_title']} ({m['type']})\"\n",
    "        if m.get(\"url\"): cite += f\"  [{m['url']}]\"\n",
    "        print(f\"  {h['score']:.3f} :: {cite}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffaf296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist FAISS + metadata to the repo root (SLIDEHUNTER/) \n",
    "from pathlib import Path\n",
    "import os, json, faiss\n",
    "\n",
    "# Resolve project base: prefer env var; else step out of notebooks/\n",
    "ENV_BASE = os.getenv(\"SLIDEHUNTER_BASE\") or os.getenv(\"SLIDEHUNT_BASE\")\n",
    "if ENV_BASE:\n",
    "    BASE = Path(ENV_BASE).resolve()\n",
    "else:\n",
    "    here = Path.cwd().resolve()\n",
    "    BASE = here.parent if here.name.lower() == \"notebooks\" else here  # run from repo root if you're inside notebooks/\n",
    "\n",
    "# Paths under the repo\n",
    "STORE_DIR  = BASE / \"data\" / \"faiss\"\n",
    "INDEX_PATH = STORE_DIR / \"canvas.index\"\n",
    "FACTS_PATH = STORE_DIR / \"facts.json\"\n",
    "STORE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save / Load helpers\n",
    "def save_store(index, facts, metas, index_path=INDEX_PATH, facts_path=FACTS_PATH):\n",
    "    \"\"\"\n",
    "    This function saves the FAISS index and its associated facts and metadata to disk.\n",
    "\n",
    "    Args:\n",
    "        index (faiss.Index): \n",
    "            The FAISS index object to be saved.\n",
    "        facts (List[str]): \n",
    "            A list of fact text snippets.\n",
    "        metas (List[dict]): \n",
    "            A list of metadata dictionaries corresponding to each fact.\n",
    "        index_path (Path or str, optional): \n",
    "            File path to save the FAISS index (default: INDEX_PATH).\n",
    "        facts_path (Path or str, optional): \n",
    "            File path to save the facts/metadata JSON (default: FACTS_PATH).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    faiss.write_index(index, str(index_path))\n",
    "    with open(facts_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"facts\": facts, \"metas\": metas}, f, ensure_ascii=False)\n",
    "    print(\"saved:\", index_path)\n",
    "    print(\"saved:\", facts_path)\n",
    "\n",
    "# Save right after you build `index`, `facts`, `metas`\n",
    "save_store(index, facts, metas)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

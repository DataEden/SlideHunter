{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "041097e2",
   "metadata": {},
   "source": [
    "# 03 â€” Tiny Evaluation\n",
    "Run a small test set (10â€“20 Qs) to sanity-check correctness and refusals.\n",
    "\n",
    "## Load store & model with auto-CUDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a447d1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Tell Hugging Face to skip TensorFlow/Flax so they never import TensorFlow (TF).\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "\n",
    "# Quiet TF logs if something still pulls it in.\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # 1=INFO, 2=WARNING, 3=ERROR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d12ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"When does phase 2 begin?\",\n",
    "    \"Any way of saying June 9th?\",\n",
    "    \"Where can I find my instructor's email?\",\n",
    "    \"Under Course Team Contact Information?\",\n",
    "    \"What was the last TLAB about?\",\n",
    "    \"An explanation of making a music recommender.\",\n",
    "    \"What lecture slides do we review pivot tables?\",\n",
    "    \"What lecture slides did we learn about control flow?\",\n",
    "    \"Can you give me a bullet point list of the most important concepts covered about SQL?\",\n",
    "    \"I'd like to know when pahse 2 commences, so I can prepare, thanks!\",\n",
    "    \"Give me a summary of P2W2's material.\",\n",
    "    \"Where did we define precision vs. recall?\",\n",
    "    \"Explain the linear regression formula.\",\n",
    "    \"What are the steps for PCA?\",\n",
    "    \"What's the difference between an L1 and L2 penalty?\",\n",
    "    \"Where can I find information on?\",\n",
    "    \"So I want to know more about XGboost and its trade-offs with AdaBoost.\",\n",
    "    \"Which slides describe the bias-variance trade-off (summary + cite both pages)?\",\n",
    "    \"pls where did u all show that 'log loss' thing ??? and that \\\"slide w/ the blue curve comparing roc stuffâ€”where?\\\"\",\n",
    "    \"Can I see other students grades??\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504401c1",
   "metadata": {},
   "source": [
    "ðŸ§ª Evaluate a list of prompts (latency + scope + citation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1938062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate prompts with auto-routing and top-1 citation\n",
    "import time, pandas as pd\n",
    "\n",
    "\n",
    "# Evaluate a lis of test queries againts the search() retriever\n",
    "def evaluate_prompts(prompts, k=4, scope=\"auto\", save_csv=True):\n",
    "    rows = []\n",
    "    for q in prompts:\n",
    "        t0 = time.time()                         # starts the timer\n",
    "        sc, hits = search(q, k=k, scope=scope)   # uses search() function with router\n",
    "        dt = time.time() - t0 # seconds\n",
    "\n",
    "        if hits:                            # if search returned results\n",
    "            h0 = hits[0]                    # take the top result\n",
    "            m = h0[\"meta\"]                  # metadata for the top result\n",
    "            # Build a human-readable citation string\n",
    "            cite = f\"{m['course_name']} â€º {m['module_name']} â€º {m['item_title']} ({m['type']})\"\n",
    "            if m.get('url'): cite += f\" [{m['url']}]\"\n",
    "            rows.append({\n",
    "                \"query\": q,\n",
    "                \"scope\": sc,                # which domain the router chose (career/technical)\n",
    "                \"latency_s\": round(dt, 3),  # search time\n",
    "                \"top_score\": round(h0[\"score\"], 3), #similarity score of top hit\n",
    "                \"top_domain\": m.get(\"domain\"),      # domain label from metadata\n",
    "                \"citation\": cite                    # readable reference for top hit\n",
    "            })\n",
    "        else:  # no hits found\n",
    "            rows.append({\n",
    "                \"query\": q, \"scope\": sc, \"latency_s\": round(dt, 3),\n",
    "                \"top_score\": None, \"top_domain\": None, \"citation\": \"(no hits)\"\n",
    "            })\n",
    "    # convert results to Dataframe for an easy display and saving\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # pretty print the DF\n",
    "    with pd.option_context(\"display.max_colwidth\", 80, \"display.width\", 120):\n",
    "        print(df.to_string(index=False))\n",
    "\n",
    "    if save_csv:\n",
    "        # save results as a CSV inside a folder\n",
    "        outdir = os.path.join(BASE, \"outputs\")\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "        path = os.path.join(outdir, \"eval_prompts.csv\")\n",
    "        df.to_csv(path, index=False)\n",
    "        print(\"\\n saved:\", path)\n",
    "\n",
    "    # quick summary\n",
    "    coverage = (df[\"citation\"] != \"(no hits)\").mean()\n",
    "    avg_score = df[\"top_score\"].dropna().mean() if df[\"top_score\"].notna().any() else None\n",
    "    print(f\"\\nCoverage: {coverage:.0%}   Avg top_score: {None if avg_score is None else round(avg_score,3)}\")\n",
    "    print(\"By domain (top-1):\")\n",
    "    print(df[\"top_domain\"].value_counts(dropna=True))\n",
    "\n",
    "    return df\n",
    "\n",
    "# run on our test_prompts evaluation\n",
    "_ = evaluate_prompts(test_prompts, k=4, scope=\"auto\", save_csv=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
